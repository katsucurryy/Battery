!pip install -q mat4py

from __future__ import absolute_import, division, print_function, unicode_literals
import mat4py as mpy
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split as tts
import h5py as st
import scipy.io



import pathlib

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import math
import csv
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator


!pip install -q git+https://github.com/tensorflow/docs 

import tensorflow_docs as tfdocs
import tensorflow_docs.plots
import tensorflow_docs.modeling

#tf version should be 2.5 or higher
tf.__version__

from google.colab import drive
drive.mount('/content/gdrive')

def array_shuffler(array_1, array_2):
    # function to shuffle two arrays of equal length simultaneously
    # while maintaining correlation between individual rows
    assert len(array_1) == len(array_2)  # confirm equal lengths
    # generate random permutation of rows up to array size
    p = np.random.permutation(len(array_1))
    # slice both arrays with the same permutation matrix
    return array_1[p], array_2[p]

def generate_dataset_from_rawfile(datapath, dataname, savepath, shuffle=True):
    # function to take the "test/train.mat" files generated by the MATLAB script
    # and split the structure into feature arrays and label arrays
    # with an option to shuffle array rows

    # datapath = The full file path of the .mat file to be used as input
    # dataname = The struct variable name present in the .mat file
    # savepath = The full file path of the location for saving the arrays
    # shuffle  = If true, shuffles the feature and label array simultaneously

    # temporary dictionary from .mat file
    data_loader = mpy.loadmat(datapath)[dataname]
    # Generate pandas data frame 'data' from the .mat file
    data = pd.DataFrame.from_dict(data_loader)

    number_of_rows = len(data)                                  # number of rows of the data frame
    number_of_samples = 0                                       # variable for number of samples in each input row
    number_of_targets = 0                                       # variable for number of samples in each target row
    inputlist = []                                              # initiate input array as list
    targetlist = []                                             # initiate target array as list

    # Extract each row from the data frame
    for row in range(number_of_rows):
        input_row = data.iloc[row, -2]                          # extract capacity history as input
        number_of_samples = len(input_row)                      # number of samples in the current input row
        current_input_row = np.zeros((number_of_samples, 1))    # initiate current input row as numpy array
        for sample in range(number_of_samples):
            # populate current row with sample values from capacity history
            current_input_row[sample, 0] = input_row[sample]
        inputlist.append(current_input_row)                     # append current row to input list

        target_row = data.iloc[row, -1]                         # extract future capacity degradation curve as output
        number_of_targets = len(target_row)
        targetlist.append(target_row)                           # append current target row to target list

    # convert input and target lists to numpy arrays and assert proper dimensions
    input_array = np.asarray(inputlist).reshape((number_of_rows, number_of_samples, 1))
    target_array = np.asarray(targetlist).reshape((number_of_rows, number_of_targets, 1))

    if shuffle:
        # shuffle input and target arrays in tandem if shuffle = true in function parameters
        features_file, labels_file = array_shuffler(input_array, target_array)
        print('Shuffled set', end=' - ')
    else:
        # If shuffle = false, then just pass the original arrays
        print('Non Shuffled set', end=' - ')
        features_file, labels_file = input_array, target_array

    print(features_file.shape, labels_file.shape)             # confirm proper dimensions of the input and target files

    # save the features and labels in appropriate locations
    featurespath = savepath + 'features.h5'
    labelspath = savepath + 'labels.h5'
    with st.File(featurespath, 'w') as storedata:
        storedata.create_dataset("features", data=features_file)
    with st.File(labelspath, 'w') as storedata:
        storedata.create_dataset("labels", data=labels_file)

    return features_file, labels_file


filepath_of_datasets = '/content/gdrive/MyDrive/ML project/'                            # Enter full file path of the .mat files of the dataset
file_matlab_nametag = 'Degradation_Prediction_Dataset_ISEA.mat'       # Enter the version tag of the file generated in MATLAB
output_nametag = 'Lifetime_Prediction_Benchmark'             # Enter the output tag of the files to be saved
training_structure_variable_name = 'Train_Set'               # Variable name of the training set in the .mat structure
testing_structure_variable_name = 'Test_Set'                 # Variable name of the testing set in the .mat structure  

# generate the full file path and names of the .mat files to be used as input to the functions
matlab_trainingset_filepath = '/content/gdrive/MyDrive/ML project/Battery_train_set.mat'
matlab_testingset_filepath = '/content/gdrive/MyDrive/ML project/Battery_test_set.mat'

# generate the full file path for saving the features and labels arrays from the functions
training_arrays_save_path = filepath_of_datasets + output_nametag + '_Train_'
testing_arrays_save_path = filepath_of_datasets + output_nametag  + '_Test_'

# call the functions to generate the arrays
X_Data, y_Data = generate_dataset_from_rawfile(matlab_trainingset_filepath, training_structure_variable_name, training_arrays_save_path, shuffle=True)
X_Test, y_Test = generate_dataset_from_rawfile(matlab_testingset_filepath, testing_structure_variable_name, testing_arrays_save_path, shuffle=False)


# split the training array further for training and validation while training each epoch
# This part can be ignored if training-validation split is done directly inside the model
training_validation_fraction = 0.15            # fraction of samples to be used as validation while training
# The above variable is a percentage to fraction, and thus must be between 0 to 1

X_train, X_val, y_train, y_val = tts(X_Data, y_Data, test_size=training_validation_fraction, shuffle=True)
print('--- shuffled split as: ', X_train.shape[0], ' samples, and testing: ', X_val.shape[0], 'samples ---')



import os

BATCH_SIZE = 256
BUFFER_SIZE = 10000

train_univariate = tf.data.Dataset.from_tensor_slices((X_train, y_train))
train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()

val_univariate = tf.data.Dataset.from_tensor_slices((X_train, y_train))
val_univariate = val_univariate.batch(BATCH_SIZE).repeat()


simple_lstm_model = tf.keras.models.Sequential([
    tf.keras.layers.LSTM(8, input_shape=X_train.shape[-2:]),
    tf.keras.layers.Dense(1)                                           

])
simple_lstm_model.compile(optimizer='adam', loss = 'mae')

E_interval = 200
EPOCHS = 10

simple_lstm_model.fit(train_univariate, epochs=EPOCHS,
                      steps_per_epoch=E_interval,
                      validation_data=val_univariate, validation_steps=50)

def create_time_steps(length):
    return list(range(-length, 0))

def show_plot(plot_data, delta, title):
    labels = ['History', 'True Future', 'Model Prediction']
    marker = ['.-', 'rx', 'go']
    time_steps = create_time_steps(plot_data[0].shape[0])
    if delta:
        future = delta
    else:
        future = 0

    plt.title(title)
    for i, x in enumerate(plot_data):
        if i:
            plt.plot(future, plot_data[i], marker[i], markersize=10,
                   label=labels[i])
        else:
            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])
        plt.legend()
        plt.xlim([time_steps[0], (future+5)*2])
        plt.xlabel('Time-Step')
    
    return plt

print(X_train[1][0])
print('y')
print(y_train[1][1])

show_plot([X_train[1], y_train[1][0]], 0, 'Sample Example')

for x, y in val_univariate.take(3):
  plot = show_plot([x[2].numpy(), y[2][0].numpy(),
                    simple_lstm_model.predict(x)[0]], 0, 'Simple LSTM model')
  plot.show()

!mkdir -p saved_model
model.save('basemodel') 


model.summary()
saved_model_dir = '/content/gdrive/MyDrive/kaggle_dataset/sign-language-mnist/basemodel'

converter = tf.lite.TFLiteConverter.from_keras_model(model)

tflite_model = converter.convert()

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model_quant = converter.convert()

import pathlib

tflite_models_dir = pathlib.Path('/content/gdrive/MyDrive/kaggle_dataset/sign-language-mnist')
tflite_models_dir.mkdir(exist_ok=True, parents=True)

tflite_model_file = tflite_models_dir/"sign_mnist.tflite"
tflite_model_file.write_bytes(tflite_model)
                                    
tflite_model_quant_file = tflite_models_dir/"sign_mnist_quant_dyn.tflite"
tflite_model_quant_file.write_bytes(tflite_model_quant)

# Helper function to evaluate a TFLite model on all images
def evaluate_model(tflite_file, model_type):
  global test_images
  global test_labels

  test_image_indices = range(test_images.shape[0])
  predictions = run_tflite_model(tflite_file, test_image_indices)

  accuracy = (np.sum(test_labels== predictions) * 100) / len(test_images)

  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (
      model_type, accuracy, len(test_images)))

from pathlib import Path
PATH_DIR = Path.cwd()
dataset_dir = PATH_DIR.joinpath('/content/gdrive/MyDrive/kaggle_dataset')


# Helper function to run inference on a TFLite model
def run_tflite_model(tflite_file, test_image_indices):
  global test_images

  # Initialize the interpreter
  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))
  interpreter.allocate_tensors()

  input_details = interpreter.get_input_details()[0]
  output_details = interpreter.get_output_details()[0]

  predictions = np.zeros((len(test_image_indices),), dtype=int)
  for i, test_image_index in enumerate(test_image_indices):
    test_image = test_images[test_image_index]
    test_label = test_labels[test_image_index]

    # Check if the input type is quantized, then rescale input data to uint8
    if input_details['dtype'] == np.uint8:
      input_scale, input_zero_point = input_details["quantization"]
      test_image = test_image / input_scale + input_zero_point

    test_image = np.expand_dims(test_image, axis=0).astype(input_details["dtype"])
    interpreter.set_tensor(input_details["index"], test_image)
    interpreter.invoke()
    output = interpreter.get_tensor(output_details["index"])[0]

    predictions[i] = output.argmax()

  return predictions

evaluate_model(tflite_model_file, model_type="Float")
evaluate_model(tflite_model_quant_file, model_type="Quantized")



def representative_dataset():
  for data in tf.data.Dataset.from_tensor_slices((test_images)).batch(1).take(100):
    yield [tf.dtypes.cast(data, tf.float32)]


converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.representative_dataset = representative_dataset
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,
tf.lite.OpsSet.TFLITE_BUILTINS]
tflite_model_int16x8 = converter.convert()

tflite_model_int16x8_file = tflite_models_dir/"sign_mnist_quant_int16x8.tflite"
tflite_model_int16x8_file.write_bytes(tflite_model_int16x8)

evaluate_model(tflite_model_int16x8_file, model_type="Quantized")

!pip install -q tensorflow-model-optimization


cluster_weights = tfmot.clustering.keras.cluster_weights
CentroidInitialization = tfmot.clustering.keras.CentroidInitialization

clustering_params = {
  'number_of_clusters': 16,
  'cluster_centroids_init': CentroidInitialization.LINEAR
}

# Cluster a whole model
clustered_model = cluster_weights(model, **clustering_params)

# Use smaller learning rate for fine-tuning clustered model
opt = tf.keras.optimizers.Adam(learning_rate=1e-5)

clustered_model.compile(
  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  optimizer=opt,
  metrics=['accuracy'])

clustered_model.summary()

# Fine-tune model
clustered_model.fit(
  train_images,
  train_labels,
  batch_size=500,
  epochs=1,
  validation_split=0.1)

_, baseline_model_accuracy = model.evaluate(
    test_images, test_labels, verbose=0)

print('Baseline test accuracy:', baseline_model_accuracy)

_, clustered_model_accuracy = clustered_model.evaluate(
  test_images, test_labels, verbose=0)

print('Baseline test accuracy:', baseline_model_accuracy)
print('Clustered test accuracy:', clustered_model_accuracy)

clustered_model.save('cluster')

ls -lh {tflite_models_dir}

import tempfile
%load_ext tensorboard

base_model = tf.keras.models.load_model('basemodel')

def apply_pruning_to_dense(layer):
  if isinstance(layer, tf.keras.layers.Dense):
    return tfmot.sparsity.keras.prune_low_magnitude(layer)
  return layer

# Use `tf.keras.models.clone_model` to apply `apply_pruning_to_dense` 
# to the layers of the model.
model_for_pruning = tf.keras.models.clone_model(
    base_model,
    clone_function=apply_pruning_to_dense,
)

model_for_pruning.summary()

model_for_pruning.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

_, model_for_pruning_accuracy = model_for_pruning.evaluate(
   test_images, test_labels, verbose=0)

print('Baseline test accuracy:', baseline_model_accuracy) 
print('Pruned test accuracy:', model_for_pruning_accuracy)

!mkdir -p saved_model
model_for_pruning.save('pruned') 
